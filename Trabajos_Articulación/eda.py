# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vDTmc7kTtLyiGVLhUDaE4vI_rrF5mIds

# Importación de datos

importar las librerias necesarias para el analisis.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from math import pi

#pip install pandas numpy matplotlib seaborn scikit-learn scipy

"""Inicialmente se carga el dataset usando pandas.

Ahora para poder comprender mejor el dataset se listarán algunas de sus caracteristicas, inicialmente la cabecera del dataset
"""

df = pd.read_csv('/content/AirQuality.csv', sep =";", decimal=",", header =0)
df.head()

"""Luego se tiene la información del tipo de dato de cada una de las columnas"""

df.info()

"""Ahora los ultimos registros del dataset"""

df.tail()

"""y finalmente las diemnsiones del dataset, filas X columnas"""

df.shape

df.describe()

"""Las columnas 15 y 16 no presentan información alguna, así que se procederá a eliminarlas.
Por alguna razon, se presentan valores negativos para las concentraciones, esos valores negativos no tienen sentido físico (las concentraciones no pueden ser negativas). Probablemente sean marcas de error usadas por el sensor (como -200).

Por tanto hay que corregirlo, para este caso particular he decidido reemplazar los valores no validos mediante imputacion usando el algoritmo del vecino mas cercano.
"""

df.drop(columns=['Unnamed: 15', 'Unnamed: 16'], inplace=True)
df.dropna(inplace=True)
df.replace(to_replace=-200, value=np.nan, inplace=True)
df.drop(columns=['NMHC(GT)'], inplace=True)
col = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)',
       'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']

imputer = KNNImputer(n_neighbors=5)
df[col] = imputer.fit_transform(df[col])

"""Se puede apreciar que las columnas 15 y 16 no aportan ningun tipo de información, por tanto es mejor no tomarlas en cuenta, asi que se procedió a eliminarlas usando el "drop".

además de eliminar los registros vacios con dropna
"""

df.shape

"""# Preprocesamiento de datos
Ahora bien, para terminar de preparar el dataset para el análisis es necesario convertir las columnas marcadas previamente como objects a su tipo de dato  correspondiente.

En este caso se tienen date y time, pero se necesita tener el formato datetime, especialmente si se requiere detectar patrones horarios u ordenar por tiempo.
"""

df['Date'] = pd.to_datetime(df['Date'],dayfirst=True)
df['Time'] = pd.to_datetime(df['Time'],format= '%H.%M.%S' ).dt.time

"""# Análisis univariado de variables numéricas
Ya que no exísten valores nulos en las variables, se procede con el análisis univariado del dataset. Para empezar, se observan las distribuciones de las variables numéricas contínuas

Posteriormente se hace el análisis exploratorio univariado, y su propósito es:

*   Detectar asimetrías, valores atípicos, modas múltiples o distribuciones extrañas.
*   Conocer la forma de la distribución.
*   Evaluar si sería útil aplicar transformaciones.
"""

for var in col:
    print(f'\nResumen de: {var}')
    print(df[var].describe())
    sns.histplot(df[var], kde=True)
    plt.title(f'Distribución de {var}')
    plt.xlabel(var)
    plt.ylabel('Frecuencia')
    plt.show()

"""# Análisis Bivariable
Inicialmente se crea la matriz de correlacion entre las variables numericas para mostrar el grado de correlación (relación lineal) entre pares de variables numéricas. Los valores van de:


*   +1: correlación positiva perfecta (ambas variables aumentan juntas).
*  0: no hay correlación lineal.
*   –1: correlación negativa perfecta (una sube, la otra baja).


"""

corr_matrix = df[col].corr()
plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Matriz de correlación entre variables numéricas')
plt.show()

"""Basado en la matriz de correlacion, se han seleccionado pares de variables para graficar y así observar de manera mas especifica su relación"""

pairs = [
    ('CO(GT)', 'PT08.S1(CO)'),
    ('C6H6(GT)', 'PT08.S2(NMHC)'),
    ('NOx(GT)', 'PT08.S3(NOx)'),
    ('NO2(GT)', 'PT08.S4(NO2)'),
    ('T', 'AH'),
]

for x, y in pairs:
    plt.figure(figsize=(6, 4))
    sns.scatterplot(data=df, x=x, y=y)
    plt.title(f'Relación entre {x} y {y}')
    plt.xlabel(x)
    plt.ylabel(y)
    plt.tight_layout()
    plt.show()

"""En este caso, el dataset contiene registros de calidad del aire tomados a lo largo de varios días, por lo que es útil realizar un análisis temporal para entender:


*   Tendencias generales.
*   Patrones diarios o semanales.
*   Estacionalidad o cambios cíclicos.

"""

df['day_of_week'] = df['Date'].dt.day_name()
sns.boxplot(data=df, x='day_of_week', y='CO(GT)')
plt.title('Distribución de CO(GT) por día de la semana')
plt.xticks(rotation=45)
plt.show()

"""Se graficó la evolución diaria del monóxido de carbono (CO(GT)), promediado por día, para observar la tendencia general a lo largo del tiempo:"""

df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str))
df.set_index('datetime', inplace=True)

df['CO(GT)'].resample('D').mean().plot(figsize=(10,4))
plt.title('Evolución diaria de CO(GT)')
plt.ylabel('Concentración')
plt.show()

df['day_of_week'] = df['Date'].dt.day_name()
sns.boxplot(data=df, x='day_of_week', y='CO(GT)')
plt.title('Distribución de CO(GT) por día de la semana')
plt.xticks(rotation=45)
plt.show()

"""# Análisis multivariado




PC1 representa la dirección de máxima varianza en los datos.

La separación entre días (ej: "Monday" en 12.5 vs "Sunday" en 5.0) sugiere que hay diferencias sistemáticas en las mediciones de contaminantes según el día.

Posibles causas:

Patrones semanales: Mayor contaminación los días laborales (Monday-Thursday) vs. fines de semana (Saturday-Sunday).

Actividad humana: Tráfico, industrias, etc., que varían por día.
"""

df['day_of_week_num'] = df.index.dayofweek
stats = df.groupby('day_of_week_num')[col].mean().reset_index()

stats_norm = stats[col].apply(lambda x: (x - x.min()) / (x.max() - x.min()))

categories = stats_norm.columns.tolist()
N = len(categories)
angles = [n / float(N) * 2 * pi for n in range(N)]
angles += angles[:1]

fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'polar': True})
ax.set_theta_offset(pi / 2)
ax.set_theta_direction(-1)

for day in stats['day_of_week_num'].unique():
    values = stats_norm[stats['day_of_week_num'] == day].values.flatten().tolist()
    values += values[:1]
    ax.plot(angles, values, linewidth=1, label=f"{['Lun','Mar','Mié','Jue','Vie','Sáb','Dom'][day]}")
    ax.fill(angles, values, alpha=0.1)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories)
plt.title("Perfil de Contaminación por Día (Normalizado)")
plt.legend(loc='upper right')
plt.show()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[col])

#  PCA conservando el 95% de la varianza
pca = PCA(n_components=0.95)
components = pca.fit_transform(X_scaled)

#  Cargar vectores de carga (loadings)
loadings = pca.components_[:2, :].T
contributions = np.sum(loadings**2, axis=1)  # Magnitud de contribución de cada variable

#  Top 5 variables más influyentes
top_idx = np.argsort(contributions)[-5:]

#  Graficar biplot simplificado
plt.figure(figsize=(12, 8))
sns.scatterplot(x=components[:, 0], y=components[:, 1], hue=df['day_of_week'], palette='Set2', alpha=0.5)

#  Agregar flechas y etiquetas de las variables más influyentes
for i in top_idx:
    plt.arrow(0, 0, loadings[i, 0]*8, loadings[i, 1]*8, color='red', alpha=0.7, head_width=0.2)
    # Use 'col' to access the column names
    plt.text(loadings[i, 0]*9, loadings[i, 1]*9, col[i], color='red', fontsize=12)

#  Etiquetas y estilo
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
plt.title('Biplot simplificado con variables más influyentes')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Para entender las relaciones multivariadas entre las variables del dataset, se aplicó un Análisis de Componentes Principales (PCA). Este método permite reducir la dimensionalidad de los datos, manteniendo la mayor parte de la varianza original, y facilitando su visualización e interpretación. A partir del PCA, se obtuvo el siguiente biplot, que representa los dos primeros componentes principales (PC1 y PC2), los cuales explican conjuntamente una parte significativa de la variabilidad total del conjunto de datos.

Puntos de colores:
Cada punto representa una observación (registro temporal) del conjunto de datos, coloreada según el día de la semana. Si bien no se observa una separación clara por día, la distribución puede indicar tendencias generales de comportamiento ambiental.

Ejes principales (PC1 y PC2):
El eje horizontal (PC1) y el eje vertical (PC2) representan combinaciones lineales de las variables originales, elegidas para capturar la mayor variabilidad posible.

PC1 explica el mayor porcentaje de la varianza.

PC2 explica el segundo mayor porcentaje, de forma ortogonal (no correlacionada) con PC1.

Flechas rojas (cargas de las variables):
Representan el peso o influencia de cada variable original en las nuevas dimensiones. Las variables con vectores más largos y alejados del centro son las que más influyen en la estructura global de los datos.
Entre las variables más influyentes se destacan:

NOx(GT) y NO2(GT): Ambas contribuyen fuertemente en dirección opuesta a las demás, indicando una posible relación inversa con otras variables.

AH (Humedad absoluta) y PT08.S4(NO2): Se orientan en dirección opuesta a los óxidos de nitrógeno, lo que sugiere una correlación negativa con estos contaminantes.

#ANOVA
"""

from scipy.stats import f_oneway

# Agrupar CO(GT) por día de la semana
lunes = df[df['day_of_week'] == 'Monday']['CO(GT)']
martes = df[df['day_of_week'] == 'Tuesday']['CO(GT)']
miercoles = df[df['day_of_week'] == 'Wednesday']['CO(GT)']
jueves = df[df['day_of_week'] == 'Thursday']['CO(GT)']
viernes = df[df['day_of_week'] == 'Friday']['CO(GT)']
sabado = df[df['day_of_week'] == 'Saturday']['CO(GT)']
domingo = df[df['day_of_week'] == 'Sunday']['CO(GT)']

# Prueba ANOVA de un solo factor
f_stat, p_val = f_oneway(lunes, martes, miercoles, jueves, viernes, sabado, domingo)

print("Estadístico F:", f_stat)
print("Valor p:", p_val)

"""Gráfico de violín para la anova de un solo factor"""

sns.violinplot(data=df, x='day_of_week', y='CO(GT)', order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
plt.title("Distribución de CO(GT) por día de la semana (Gráfico de Violín)")
plt.xticks(rotation=45)
plt.ylabel("Concentración de CO (mg/m^3)")
plt.grid(True)
plt.show()

"""Anova multi factor"""

# Convertir Time en datetime.time y crear tramos horarios
def categorizar_hora(t):
    if t.hour < 12:
        return 'Mañana'
    elif 12 <= t.hour < 18:
        return 'Tarde'
    else:
        return 'Noche'

df['tramo_horario'] = df['Time'].apply(lambda t: categorizar_hora(t))

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Modelo ANOVA de dos factores
modelo = ols('Q("CO(GT)") ~ C(day_of_week) + C(tramo_horario) + C(day_of_week):C(tramo_horario)', data=df).fit()
anova_tabla = sm.stats.anova_lm(modelo, typ=2)

print(anova_tabla)